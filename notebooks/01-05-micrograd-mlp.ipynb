{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393849ec-37fd-4938-8d68-7f2ccbf58304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f497be-81fa-4666-a0d0-3917b007004f",
   "metadata": {},
   "source": [
    "# Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c0434-fd34-4368-a598-bf7384fb1fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op: str = None, label: str = \"\"):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self._backward: callable = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = (\n",
    "            other if isinstance(other, Value) else Value(other)\n",
    "        )  # Supporting add of integers\n",
    "        out = Value(self.data + other.data, _children=(self, other), _op=\"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return self.data > other.data\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, _children=(self, other), _op=\"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):  # int + object\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, int | float)\n",
    "        out = Value(self.data**other, (self,), _op=f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = out.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, _children=(self,), _op=\"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):  # Needs to be reversed because we start at the end\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886592b3-7e71-4cb1-a5bb-87d9821cb541",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d847162-8f30-45b0-b9bb-fcfafc52f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the computation graph\n",
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "# for any value use a rectangle, for any operation use a circle\n",
    "def draw_graph(value: Value):\n",
    "    dot = Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})  # Left to right\n",
    "    nodes, edges = trace(value)\n",
    "    # For each node, add a rectangle with the value\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(\n",
    "            name=uid,\n",
    "            label=\"{%s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad),\n",
    "            shape=\"record\",\n",
    "        )\n",
    "        # For any operation, use a circle\n",
    "        if n._op:\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            # Add edges to the graph\n",
    "            dot.edge(uid + n._op, uid)\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8de5c-bde4-476e-99dd-02648ae0f046",
   "metadata": {},
   "source": [
    "# Building a full Neural Network (aka. Multi-Layer Perceptron)\n",
    "Now that we have the ``Value`` objects and the computational graph, we would like to build out an actual neural network (Multi-Layer Perceptron).\n",
    "\n",
    "- An MLP consists of multiple layers\n",
    "- A layer is a stack of multiple neurons\n",
    "- A neurons consists of multiple inputs and a matching number of weights, plus a bias node\n",
    "\n",
    "<img src='../img/neural_net.webp' alt='Example Multi Layer Percepton'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5967e-4d3c-43a1-9844-0ce2d7f50a54",
   "metadata": {},
   "source": [
    "## Neuron\n",
    "A neuron takes a fixed number of inputs and multiplies them with an equal amount of weights, it adds a bias to it which will then pass through an activation function\n",
    "$$\n",
    "o = f (\\sum_{i=1}^n x_i w_i + b)\n",
    "$$\n",
    "<img src='../img/neuron_model-cs231n.jpeg' alt='Example Neuron'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304899b-4dd3-4faa-923e-902528610681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(np.random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(np.random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum(xi * wi for xi, wi in zip(x, self.w)) + self.b\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4afa90-ced3-4159-b2c6-ef9916c137f8",
   "metadata": {},
   "source": [
    "## Layer\n",
    "A layer is a set of neurons stacked on top of each other with the input passing through every neuron individually and producing a fixed number of outputs ``nout``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d4c9f-7fa5-4546-8e33-462b795c9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        param = []\n",
    "        for neuron in self.neurons:\n",
    "            param.extend(neuron.parameters())\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2d5d6-2684-4e56-aca6-f72cfa7542b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0]\n",
    "l = Layer(nin=2, nout=4)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c7b12-028a-4337-bbd9-73f953e8a33b",
   "metadata": {},
   "source": [
    "We then get four outputs, for the four neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0688261-565a-43e7-9831-c32963e0de00",
   "metadata": {},
   "source": [
    "## MLP\n",
    "An MLP is number of layer in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506e886-13c9-411e-a782-d5db9ce81de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nin: int, nouts: list[int]):\n",
    "        sz_layer = [nin] + nouts\n",
    "        self.layers = [Layer(sz_layer[i], sz_layer[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        param = []\n",
    "        for layer in self.layers:\n",
    "            param.extend(layer.parameters())\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51fdf3-2682-4a3c-9062-f4c133e19082",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0, 4.0]\n",
    "mlp = MLP(3, [4, 4, 1])\n",
    "y = mlp(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfe6b7-a41e-4648-84b2-102125e6cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec66a18-0ece-4d1d-b325-b87947a16290",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e7240-08fb-43b8-bb88-fe82c9ec7731",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "We now want to create an example that shows how an MLP is able to learn a pattern by implementing the standard algorithm used to optimize neural nets: **Stochastic Gradient Descent (SGD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310ef1c-6b02-452e-9c99-0df4fffa997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [[2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0]]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # desired targets\n",
    "\n",
    "model = MLP(3, [4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2b504-eb7b-4499-8f6a-685abd8e2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypreds = [model(x) for x in xs]\n",
    "ypreds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59601bf9-7388-41af-a543-b1acbda5e147",
   "metadata": {},
   "source": [
    "We now need to determine how close our predictions align with the desired targets. There are multiple ways of doing this most straightforward is by computing the difference between the predictions and targets.\n",
    "\n",
    "It is important though, that we can guarantee that out loss is positive, because otherwise we would have a *negative loss* which would indicate a gain.\n",
    "Any operation that removes a possible $-$ sign is well suited. \n",
    "\n",
    "Most straightforward and often used is the **Mean-Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (\\text{pred} - \\text{targets})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6c02b-d3c1-40e5-b760-03c7bc73e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda pred, label: sum([(xs - ys) ** 2 for xs, ys in zip(pred, label)]) / len(\n",
    "    pred\n",
    ")\n",
    "loss = mse(ypreds, ys)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f41ac0-4e3e-4919-956e-89b77f8a1440",
   "metadata": {},
   "source": [
    "Since the outputs of the MLP are Value objects as well computing the loss on the difference between the prediction and the targets will yield a Value objects as well containing the loss.\n",
    "\n",
    "The value object is therefore attached at the end of the computational graph and we can call ``.backward()`` on it, which will kick off backpropagation, computing the loss with respect to the individual parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f72c44-bfc7-4da3-8bec-b33494979905",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bf737-d018-4697-a3c4-af73cf9b5270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_graph(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be53f5e-016c-4a2d-a8e8-14768fd1c929",
   "metadata": {},
   "source": [
    "Since we now have the gradients, indicating what variable influence the output by how much we can start changing the values of the weights.\n",
    "\n",
    "We nudged every weight by the gradient, but only by a small amount so that we can progress slowly, we therefore introduce a new parameter ``alpha`` which will be out **learning rate**.\n",
    "\n",
    "We then nudged the parameters by decreasing the data by alpha times the gradient.\n",
    "\n",
    "It is important to note that we **decrease** not increasing. This is because of the influence the gradients of the weights have on the output.\n",
    "\n",
    "Considering that our final output is now our **loss**, and we want to decrease the loss we need to decrease the weights by their gradients, to get the output (loss) down as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2eebc-7e52-469a-a26d-545d80141c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "\n",
    "for _ in range(2000):\n",
    "    ypreds = [model(x) for x in xs]\n",
    "\n",
    "    # Zero the gradients because of gradient accumulation\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss = mse(ypreds, ys)\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data -= alpha * p.grad\n",
    "\n",
    "print(ypreds)\n",
    "print(f\"Loss {loss.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b16fc-edf4-41f2-a1dd-d4790f995c0a",
   "metadata": {},
   "source": [
    "Tuning the learning rate is an art of it's own, and there are many tools available that help with it.\n",
    "\n",
    "There are also more advanced optimization algorithms such as **RMS Prob**, **Adam**, etc.\n",
    "However, those only are optimization of the standard **SGD** they do not tackle the problem from a different angle. If you strip away all their optimizations you end up with Stochastic Gradient Descent\n",
    "\n",
    "In other words:\n",
    "\n",
    "**If you understand SGD you understand the biggest part of training neural nets, as eveything else is merely optimization ;)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01275a-d83d-420f-9948-45eb6e35f9c4",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Now let's try to solve a classification problem using out own NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20043479-4077-4990-8f49-1905d4bf64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, shuffle=True, noise=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228fa8e-3774-40ce-a3f2-6da3086ad5b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81743c36-8628-4bbf-802f-904204018471",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbad29-1a21-417e-918e-80bf5fc52528",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y * 2 - 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993df05a-31e3-420d-9c20-5ed390b774ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all elements across the first dimension at position 0, which is the x-coordinate\n",
    "X[:, 0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7e58e-6395-46cf-b322-b599ce2c24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_visualize_custom(X, y, net=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\"jet\")\n",
    "\n",
    "    if net is not None:\n",
    "        x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx1, xx2 = np.meshgrid(\n",
    "            np.linspace(x1_min, x1_max, 100),\n",
    "            np.linspace(x2_min, x2_max, 100),\n",
    "        )\n",
    "        X_grid = np.stack([xx1.ravel(), xx2.ravel()], axis=1)\n",
    "        # Predict using the custom MLP\n",
    "        y_grid = []\n",
    "        for xg in X_grid:\n",
    "            out = net(xg.tolist())\n",
    "            # Output is a Value object, get its data and sign\n",
    "            y_grid.append(np.sign(out.data))\n",
    "        y_grid = np.array(y_grid).reshape(xx1.shape)\n",
    "        plt.contourf(xx1, xx2, y_grid, cmap=\"jet\", alpha=0.2)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model_predict_visualize_custom(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ab5c9-8151-4524-8145-6b2de7d49d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting to list as we don't support numpy\n",
    "xs, ys = X.tolist(), y.tolist()\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b216cbc-c301-4946-93fd-2c3d9ca5e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(2, [4, 1])\n",
    "mlp(xs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b794a28-d6c8-423e-a757-58d094d9beea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Zero the gradients\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    # Forward pass\n",
    "    pred = []\n",
    "    for x in xs:\n",
    "        pred.append(mlp(x))\n",
    "\n",
    "    loss = mse(pred, ys)\n",
    "    # Computing accuracy\n",
    "    p = [1 if p.data > 0 else -1 for p in pred]\n",
    "    acc = np.mean(np.array(p) == np.array(ys))\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    for p in mlp.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss}, Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb94009-d750-4f45-b46d-10d4d01b7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict_visualize_custom(X, y, mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
